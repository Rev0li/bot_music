# ğŸµ Web Scraper Musical

Un projet d'apprentissage pour rÃ©cupÃ©rer des informations musicales depuis des pages web et les stocker au format JSON.

## ğŸ“‹ Table des matiÃ¨res

- [Description](#description)
- [PrÃ©requis](#prÃ©requis)
- [Installation](#installation)
- [Structure du projet](#structure-du-projet)
- [Utilisation](#utilisation)
- [Configuration](#configuration)
- [Exemples](#exemples)
- [DÃ©pannage](#dÃ©pannage)
- [Contribution](#contribution)

## ğŸ“ Description

Ce projet permet de :
- âœ… RÃ©cupÃ©rer le contenu HTML de pages web
- âœ… Extraire des informations spÃ©cifiques (titres, artistes, liens, images)
- âœ… Sauvegarder les donnÃ©es au format JSON
- âœ… Organiser les fichiers de sortie
- âœ… Communiquer entre JavaScript et Python (Ã  venir)

**âš ï¸ Important :** Ce projet est Ã  des fins Ã©ducatives uniquement. Respectez toujours les conditions d'utilisation des sites web et les droits d'auteur.

## ğŸ”§ PrÃ©requis

- **Node.js** >= 14.0.0
- **npm** >= 6.0.0
- **Git** (optionnel)

### VÃ©rifier les prÃ©requis

```bash
node --version
npm --version
```

## ğŸš€ Installation

### Installation automatique (recommandÃ©e)

```bash
# Cloner le projet
git clone https://github.com/ton-username/web-scraper-musical.git
cd web-scraper-musical

# Lancer l'installation automatique
chmod +x setup.sh
./setup.sh
```

### Installation manuelle

```bash
# Initialiser le projet
npm init -y

# Installer les dÃ©pendances
npm install axios@1.6.0 cheerio@1.0.0-rc.10 fs-extra@11.1.1

# CrÃ©er la structure de dossiers
mkdir -p src data output logs config
```

## ğŸ“‚ Structure du projet

```
web-scraper-musical/
â”œâ”€â”€ ğŸ“¦ node_modules/          # DÃ©pendances (auto-gÃ©nÃ©rÃ©es)
â”œâ”€â”€ ğŸ“ src/                   # Code source
â”‚   â”œâ”€â”€ scraper.js           # Script principal de scraping
â”‚   â”œâ”€â”€ parser.js            # Fonctions de parsing HTML
â”‚   â””â”€â”€ utils.js             # Fonctions utilitaires
â”œâ”€â”€ ğŸ’¾ data/                  # DonnÃ©es brutes rÃ©cupÃ©rÃ©es
â”‚   â”œâ”€â”€ raw/                 # HTML brut
â”‚   â””â”€â”€ temp/                # Fichiers temporaires
â”œâ”€â”€ ğŸ“Š output/                # RÃ©sultats finaux
â”‚   â”œâ”€â”€ json/                # Fichiers JSON
â”‚   â””â”€â”€ reports/             # Rapports de scraping
â”œâ”€â”€ ğŸ“‹ logs/                  # Fichiers de log
â”œâ”€â”€ âš™ï¸ config/                # Configuration
â”‚   â””â”€â”€ default.json         # Configuration par dÃ©faut
â”œâ”€â”€ ğŸ“„ package.json           # DÃ©pendances et scripts
â”œâ”€â”€ ğŸš« .gitignore            # Fichiers ignorÃ©s par git
â”œâ”€â”€ ğŸ› ï¸ setup.sh              # Script d'installation
â””â”€â”€ ğŸ“– README.md             # Ce fichier
```

## ğŸ¯ Utilisation

### Scraping basique

```bash
# Aller dans le dossier src
cd src

# Lancer le scraper
node scraper.js
```

### Avec paramÃ¨tres

```javascript
// Dans ton script
const scraper = new WebScraper();

// Scraper une page
const data = await scraper.scrapeWebsite('https://example.com', {
    title: '.song-title',
    artist: '.artist-name',
    album: '.album-name'
});

// Sauvegarder en JSON
await scraper.saveToJSON('ma_musique.json');
```

## âš™ï¸ Configuration

Modifie le fichier `config/default.json` :

```json
{
  "scraper": {
    "delay": 1000,           // DÃ©lai entre requÃªtes (ms)
    "timeout": 10000,        // Timeout des requÃªtes (ms)
    "userAgent": "Mozilla/5.0...",
    "maxRetries": 3          // Nombre de tentatives
  },
  "output": {
    "format": "json",        // Format de sortie
    "directory": "./output", // Dossier de sortie
    "filename": "scraped_data.json"
  },
  "logging": {
    "level": "info",         // Niveau de log
    "directory": "./logs"    // Dossier des logs
  }
}
```

## ğŸ“š Exemples

### Exemple 1 : Scraper des informations de base

```javascript
const WebScraper = require('./src/scraper');

async function exemple1() {
    const scraper = new WebScraper();
    
    const selectors = {
        title: 'h1',
        description: '.description',
        links: 'a'
    };
    
    const data = await scraper.scrapeWebsite('https://httpbin.org/html', selectors);
    console.log(data);
}

exemple1();
```

### Exemple 2 : Scraper plusieurs pages

```javascript
async function exemple2() {
    const scraper = new WebScraper();
    const urls = [
        'https://site1.com',
        'https://site2.com',
        'https://site3.com'
    ];
    
    for (const url of urls) {
        await scraper.scrapeWebsite(url, selectors);
        // Pause respectueuse entre les requÃªtes
        await new Promise(resolve => setTimeout(resolve, 1000));
    }
    
    await scraper.saveToJSON('resultats_multiples.json');
}
```

### Exemple 3 : Filtrer et nettoyer les donnÃ©es

```javascript
async function exemple3() {
    const scraper = new WebScraper();
    
    // Scraper des donnÃ©es
    await scraper.scrapeWebsite(url, selectors);
    
    // Nettoyer les donnÃ©es
    scraper.cleanData();
    
    // Chercher dans les donnÃ©es
    const results = scraper.search('rock');
    console.log(`TrouvÃ© ${results.length} rÃ©sultats pour "rock"`);
}
```

## ğŸ§ª Tests

```bash
# Tester l'installation
node -e "console.log('Node.js fonctionne !'); require('axios'); require('cheerio'); console.log('Toutes les dÃ©pendances OK!');"

# Tester le scraper basique
cd src
node -e "const scraper = require('./scraper'); console.log('Scraper chargÃ© avec succÃ¨s!');"
```

## ğŸ› DÃ©pannage

### Erreur "Module not found"
```bash
# RÃ©installer les dÃ©pendances
rm -rf node_modules package-lock.json
npm install
```

### Erreur avec Cheerio
```bash
# Installer la version compatible
npm uninstall cheerio
npm install cheerio@1.0.0-rc.10
```

### Erreur CORS ou 403
- VÃ©rifiez le User-Agent dans la configuration
- Ajoutez des dÃ©lais entre les requÃªtes
- Respectez les robots.txt du site

### Timeout des requÃªtes
- Augmentez le timeout dans `config/default.json`
- VÃ©rifiez votre connexion internet
- Le site cible est peut-Ãªtre lent

## ğŸ“ˆ FonctionnalitÃ©s Ã  venir

- [ ] Interface Python pour traitement avancÃ©
- [ ] CrÃ©ation automatique de dossiers organisÃ©s
- [ ] Modification des mÃ©tadonnÃ©es de fichiers
- [ ] Interface web pour configuration
- [ ] Support de diffÃ©rents formats de sortie
- [ ] Planification automatique de scraping

## ğŸ¤ Contribution

1. Fork le projet
2. CrÃ©e une branche (`git checkout -b feature/nouvelle-fonctionnalite`)
3. Commit tes changements (`git commit -am 'Ajoute une nouvelle fonctionnalitÃ©'`)
4. Push sur la branche (`git push origin feature/nouvelle-fonctionnalite`)
5. Ouvre une Pull Request

## ğŸ“œ Licence

Ce projet est Ã  des fins Ã©ducatives uniquement. 

**Rappel important :** 
- Respectez les robots.txt des sites
- Ne surchargez pas les serveurs (utilisez des dÃ©lais)
- Respectez les droits d'auteur et les conditions d'utilisation
- Ce code est fourni tel quel, sans garantie

## ğŸ‘¤ Auteur

**Ton nom**
- GitHub: [@ton-username](https://github.com/ton-username)
- Email: ton-email@example.com

## ğŸ™ Remerciements

- [Axios](https://github.com/axios/axios) pour les requÃªtes HTTP
- [Cheerio](https://github.com/cheeriojs/cheerio) pour le parsing HTML
- [fs-extra](https://github.com/jprichardson/node-fs-extra) pour la gestion des fichiers
- La communautÃ© Node.js pour les ressources d'apprentissage

---

**ğŸµ Happy scraping! (lÃ©galement et respectueusement)**
